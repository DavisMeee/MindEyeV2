{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d06f9f36-0b22-4a7e-a6b8-a443a71b9b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/admin/home-mihirneal/miniconda3/envs/mindeye/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/admin/home-mihirneal/miniconda3/envs/mindeye/lib/python3.10/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import models\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "import math\n",
    "from einops import rearrange\n",
    "import utils\n",
    "import random\n",
    "import string\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display, Image\n",
    "\n",
    "import webdataset as wds\n",
    "import gc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1713b2b3-f81f-4db9-889e-5fa95260236e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: BvJzD580AL_interactive\n",
      "['--data_path=/fsx/proj-fmri/shared/mindeyev2_dataset', '--model_name=BvJzD580AL_interactive', '--subj=1', '--batch_size=128', '--blurry_recon', '--no-depth_recon', '--clip_scale=1.', '--blur_scale=100.', '--depth_scale=100.', '--max_lr=3e-4', '--mixup_pct=.66', '--num_epochs=24', '--ckpt_interval=999', '--no-use_image_aug', '--no-ckpt_saving']\n"
     ]
    }
   ],
   "source": [
    "global_batch_size = batch_size = 128\n",
    "data_type = torch.float16 # change depending on your mixed_precision\n",
    "\n",
    "if utils.is_interactive():\n",
    "    # create random model_name\n",
    "    model_name = ''.join(random.choices(string.ascii_letters + string.digits, k=10))\n",
    "    model_name = model_name + \"_interactive\"\n",
    "    print(\"model_name:\", model_name)\n",
    "\n",
    "    # global_batch_size and batch_size should already be defined in the above cells\n",
    "    jupyter_args = f\"--data_path=/fsx/proj-fmri/shared/mindeyev2_dataset \\\n",
    "                    --model_name={model_name} \\\n",
    "                    --subj=1 --batch_size={batch_size} --blurry_recon --no-depth_recon \\\n",
    "                    --clip_scale=1. --blur_scale=100. --depth_scale=100. \\\n",
    "                    --max_lr=3e-4 --mixup_pct=.66 --num_epochs=24 --ckpt_interval=999 --no-use_image_aug --no-ckpt_saving\"\n",
    "\n",
    "    jupyter_args = jupyter_args.split()#other variables can be specified in the following string:\n",
    "    \n",
    "    print(jupyter_args)\n",
    "    \n",
    "    from IPython.display import clear_output # function to clear print outputs in cell\n",
    "    %load_ext autoreload \n",
    "    # this allows you to change functions in models.py or utils.py and have this notebook automatically update with your revisions\n",
    "    %autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "357f3939-0e9b-44ea-8cb0-a78eee526c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Model Training Configuration\")\n",
    "parser.add_argument(\n",
    "    \"--model_name\", type=str, default=\"testing\",\n",
    "    help=\"name of model, used for ckpt saving and wandb logging (if enabled)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--data_path\", type=str, default=\"/fsx/proj-fmri/shared/natural-scenes-dataset\",\n",
    "    help=\"Path to where NSD data is stored / where to download it to\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--subj\",type=int, default=1, choices=[1,2,5,7],\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--batch_size\", type=int, default=32,\n",
    "    help=\"Batch size can be increased by 10x if only training v2c and not diffusion diffuser\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--wandb_log\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"whether to log to wandb\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--resume_from_ckpt\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"if not using wandb and want to resume from a ckpt\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--wandb_project\",type=str,default=\"stability\",\n",
    "    help=\"wandb project name\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--mixup_pct\",type=float,default=.33,\n",
    "    help=\"proportion of way through training when to switch from BiMixCo to SoftCLIP\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--blurry_recon\",action=argparse.BooleanOptionalAction,default=True,\n",
    "    help=\"whether to output blurry reconstructions\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--depth_recon\",action=argparse.BooleanOptionalAction,default=True,\n",
    "    help=\"whether to output depth reconstructions\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--blur_scale\",type=float,default=100.,\n",
    "    help=\"multiply loss from blurry recons by this number\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--depth_scale\",type=float,default=100.,\n",
    "    help=\"multiply loss from depth recons by this number\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--clip_scale\",type=float,default=1.,\n",
    "    help=\"multiply contrastive loss by this number\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--use_image_aug\",action=argparse.BooleanOptionalAction,default=True,\n",
    "    help=\"whether to use image augmentation\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_epochs\",type=int,default=120,\n",
    "    help=\"number of epochs of training\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--lr_scheduler_type\",type=str,default='cycle',choices=['cycle','linear'],\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--ckpt_saving\",action=argparse.BooleanOptionalAction,default=True,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--ckpt_interval\",type=int,default=5,\n",
    "    help=\"save backup ckpt and reconstruct every x epochs\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seed\",type=int,default=42,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--max_lr\",type=float,default=3e-4,\n",
    ")\n",
    "\n",
    "if utils.is_interactive():\n",
    "    args = parser.parse_args(jupyter_args)\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "\n",
    "# create global variables without the args prefix\n",
    "for attribute_name in vars(args).keys():\n",
    "    globals()[attribute_name] = getattr(args, attribute_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9feca2e9-3bea-4e17-b95b-50efa766e19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = models.DV2MLP(use_cont=True)\n",
    "m2 = models.DV2MLP(use_cont=True)\n",
    "m3 = models.DV2MLP(use_cont=True)\n",
    "m4 = models.DV2MLP(use_cont=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3cffd76-7212-4872-bf44-90d8b2abd358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckp1 = torch.load(\"/fsx/proj-fmri/mihirneal/MindEyeV2/train_logs/models/dp_1/last.pth\")\n",
    "ckp2 = torch.load(\"/fsx/proj-fmri/mihirneal/MindEyeV2/train_logs/models/dp_2/last.pth\")\n",
    "ckp3 = torch.load(\"/fsx/proj-fmri/mihirneal/MindEyeV2/train_logs/models/dp_3/last.pth\")\n",
    "ckp4 = torch.load(\"/fsx/proj-fmri/mihirneal/MindEyeV2/train_logs/models/dp_4/last.pth\")\n",
    "m1.load_state_dict(ckp1[\"model_state_dict\"])\n",
    "m2.load_state_dict(ckp2[\"model_state_dict\"])\n",
    "m3.load_state_dict(ckp3[\"model_state_dict\"])\n",
    "m4.load_state_dict(ckp4[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5ee6ed59-b137-4761-a73f-786fdfa62969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/fsx/proj-fmri/shared/mindeyev2_dataset/wds/subj01/new_train/{0..36}.tar\n",
      "/fsx/proj-fmri/shared/mindeyev2_dataset/wds/subj01/test/0.tar\n",
      "0 2770 2770\n",
      "---\n",
      "\n",
      "194 24960 24960\n"
     ]
    }
   ],
   "source": [
    "train_url = f\"{data_path}/wds/subj0{subj}/new_train/\" + \"{0..36}.tar\"\n",
    "print(train_url)\n",
    "def my_split_by_node(urls): return urls\n",
    "if subj==1:\n",
    "    num_train = 24958\n",
    "    num_test = 2770\n",
    "test_batch_size = num_test\n",
    "train_data = wds.WebDataset(train_url,resampled=False,nodesplitter=my_split_by_node)\\\n",
    "                    .shuffle(750, initial=1500, rng=random.Random(42))\\\n",
    "                    .decode(\"torch\")\\\n",
    "                    .rename(behav=\"behav.npy\", future_behav=\"future_behav.npy\")\\\n",
    "                    .to_tuple(*[\"behav\", \"future_behav\"])\n",
    "train_dl = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=False, drop_last=True, pin_memory=True)\n",
    "\n",
    "test_url = f\"{data_path}/wds/subj0{subj}/test/\" + \"0.tar\"\n",
    "print(test_url)\n",
    "\n",
    "test_data = wds.WebDataset(test_url,resampled=False,nodesplitter=my_split_by_node)\\\n",
    "                    .shuffle(750, initial=1500, rng=random.Random(42))\\\n",
    "                    .decode(\"torch\")\\\n",
    "                    .rename(behav=\"behav.npy\", future_behav=\"future_behav.npy\")\\\n",
    "                    .to_tuple(*[\"behav\", \"future_behav\"])\n",
    "test_dl = torch.utils.data.DataLoader(test_data, batch_size=test_batch_size, shuffle=False, drop_last=True, pin_memory=True)\n",
    "\n",
    "\n",
    "# ### check dataloaders are working\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "test_vox_indices = []\n",
    "test_73k_images = []\n",
    "for test_i, (behav, future_behav) in enumerate(test_dl):\n",
    "    test_vox_indices = np.append(test_vox_indices, behav[:,0,5].cpu().numpy())\n",
    "    test_73k_images = np.append(test_73k_images, behav[:,0,0].cpu().numpy())\n",
    "test_vox_indices = test_vox_indices.astype(np.int16)\n",
    "print(test_i, (test_i+1) * test_batch_size, len(test_vox_indices))\n",
    "print(\"---\\n\")\n",
    "\n",
    "train_vox_indices = []\n",
    "train_73k_images = []\n",
    "for train_i, (behav, future_behav) in enumerate(train_dl):\n",
    "    train_vox_indices = np.append(train_vox_indices, behav[:,0,5].long().cpu().numpy())\n",
    "    train_73k_images = np.append(train_73k_images, behav[:,0,0].cpu().numpy())\n",
    "train_vox_indices = train_vox_indices.astype(np.int16)\n",
    "print(train_i, (train_i+1) * batch_size, len(train_vox_indices))\n",
    "\n",
    "all_vox_indices = np.hstack((train_vox_indices, test_vox_indices))\n",
    "all_images = np.hstack((train_73k_images, test_73k_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8b50383d-c489-47e7-a984-84e6e854bb6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subj01 betas loaded into memory\n",
      "voxels torch.Size([27750, 15724])\n",
      "images torch.Size([73000, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# load betas\n",
    "f = h5py.File(f'{data_path}/betas_all_subj0{subj}.hdf5', 'r')\n",
    "# f = h5py.File(f'{data_path}/betas_subj0{subj}_thresholded_wholebrain.hdf5', 'r')\n",
    "\n",
    "voxels = f['betas'][:]\n",
    "print(f\"subj0{subj} betas loaded into memory\")\n",
    "voxels = torch.Tensor(voxels).to(\"cpu\").to(data_type)\n",
    "print(\"voxels\", voxels.shape)\n",
    "num_voxels = voxels.shape[-1]\n",
    "\n",
    "# load orig images\n",
    "f = h5py.File(f'{data_path}/coco_images_224_float16.hdf5', 'r')\n",
    "images = f['images'][:]\n",
    "images = torch.Tensor(images).to(\"cpu\").to(data_type)\n",
    "print(\"images\", images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "46ac0f81-bc0d-4782-a207-6332bfe82060",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image, test_voxel = None, None\n",
    "with torch.no_grad(), torch.cuda.amp.autocast(dtype=data_type): \n",
    "            for test_i, (behav, future_behav) in enumerate(test_dl):  \n",
    "                # all test samples should be loaded per batch such that test_i should never exceed 0\n",
    "                assert len(behav) == num_test\n",
    "                \n",
    "                ## Average same-image repeats ##\n",
    "                if test_image is None:\n",
    "                    voxel = voxels[behav[:,0,5].cpu().long()]\n",
    "                    image = behav[:,0,0].cpu().long()\n",
    "                    \n",
    "                    unique_image, sort_indices = torch.unique(image, return_inverse=True)\n",
    "                    for im in unique_image:\n",
    "                        locs = torch.where(im == image)[0]\n",
    "                        if test_image is None:\n",
    "                            test_image = images[im][None]\n",
    "                            test_voxel = torch.mean(voxel[locs],axis=0)[None]\n",
    "                        else:\n",
    "                            test_image = torch.vstack((test_image, images[im][None]))\n",
    "                            test_voxel = torch.vstack((test_voxel, torch.mean(voxel[locs],axis=0)[None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e1c2fa7-186e-4b46-b063-964ccab4baa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param counts:\n",
      "1,596,367,896 total\n",
      "1,596,367,896 trainable\n",
      "b.shape torch.Size([2, 1, 4096])\n",
      "torch.Size([2, 77, 2048]) torch.Size([2, 257, 768])\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "from diffusers.models.vae import Decoder\n",
    "class BrainNetwork(nn.Module):\n",
    "    def __init__(self, text, pool, out_dim=2048, in_dim=15724, seq_len=2, h=4096, n_blocks=4, drop=.15, clip_size=2048):\n",
    "        super().__init__()\n",
    "        self.is_text = text\n",
    "        self.is_pool = pool\n",
    "        self.seq_len = seq_len\n",
    "        self.h = h\n",
    "        self.clip_size = clip_size\n",
    "        \n",
    "        # Initial linear layer to match the input dimensions to hidden dimensions\n",
    "        # self.lin0 = nn.Linear(in_dim, seq_len * h)\n",
    "        \n",
    "        # Mixer Blocks\n",
    "        self.mixer_blocks1 = nn.ModuleList([\n",
    "            self.mixer_block1(h, drop) for _ in range(n_blocks)\n",
    "        ])\n",
    "        self.mixer_blocks2 = nn.ModuleList([\n",
    "            self.mixer_block2(seq_len, drop) for _ in range(n_blocks)\n",
    "        ])\n",
    "        \n",
    "        # Output linear layer\n",
    "        if self.is_text:\n",
    "            self.clin1 = nn.Linear(h * seq_len, out_dim, bias=True)\n",
    "            self.clin2 = nn.Linear(h * seq_len, 768*257, bias=True)\n",
    "            self.clip_proj = nn.Sequential(\n",
    "            nn.LayerNorm(768),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(768, 2048),\n",
    "            nn.LayerNorm(2048),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(2048, 2048),\n",
    "            nn.LayerNorm(2048),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(2048, 768)\n",
    "        )\n",
    "        if self.is_pool:\n",
    "            self.clin3 = nn.Sequential(\n",
    "                nn.Linear(h * seq_len, out_dim),\n",
    "                nn.LayerNorm(out_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(out_dim, 1280),\n",
    "            )\n",
    "\n",
    "        # low-rank matrices\n",
    "        # self.rank = 500\n",
    "        # self.U = nn.Parameter(torch.randn(self.rank, out_dim))\n",
    "        # self.V = nn.Parameter(torch.randn(h * seq_len, self.rank))\n",
    "        # self.S = nn.Parameter(torch.randn(out_dim))\n",
    "\n",
    "        \n",
    "\n",
    "#         self.text_proj = nn.Sequential(\n",
    "#             nn.LayerNorm(clip_size),\n",
    "#             nn.GELU(),\n",
    "#             nn.Linear(clip_size, 2048),\n",
    "#             # nn.Dropout(0.5),\n",
    "#             nn.LayerNorm(2048),\n",
    "#             nn.GELU(),\n",
    "#             nn.Linear(2048, 2048),\n",
    "#             # nn.Dropout(0.5),\n",
    "#             nn.LayerNorm(2048),\n",
    "#             nn.GELU(),\n",
    "#             nn.Linear(2048, clip_size)\n",
    "#         )\n",
    "\n",
    "#         self.pooled_proj = nn.Sequential(\n",
    "#             nn.LayerNorm(157696),\n",
    "#             nn.GELU(),\n",
    "#             nn.Linear(157696, h),\n",
    "#             nn.LayerNorm(h),\n",
    "#             nn.GELU(),\n",
    "#             nn.Linear(h, h),\n",
    "#             nn.LayerNorm(h),\n",
    "#             nn.GELU(),\n",
    "#             nn.Linear(h, 1280)\n",
    "#         )\n",
    "\n",
    "        \n",
    "    def mixer_block1(self, h, drop):\n",
    "        return nn.Sequential(\n",
    "            nn.LayerNorm(h),\n",
    "            self.mlp(h, h, drop),  # Token mixing\n",
    "        )\n",
    "\n",
    "    def mixer_block2(self, seq_len, drop):\n",
    "        return nn.Sequential(\n",
    "            nn.LayerNorm(seq_len),\n",
    "            self.mlp(seq_len, seq_len, drop)  # Channel mixing\n",
    "        )\n",
    "    \n",
    "    def mlp(self, in_dim, out_dim, drop):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(in_dim, out_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(out_dim, out_dim),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # make empty tensors for blur and depth outputs\n",
    "        \n",
    "        # Initial linear layer\n",
    "        # x = self.lin0(x)\n",
    "        \n",
    "        # Reshape to seq_len by dim\n",
    "        # x = x.reshape(-1, self.seq_len, self.h)\n",
    "        \n",
    "        # Mixer blocks\n",
    "        residual1 = x\n",
    "        residual2 = x.permute(0,2,1)\n",
    "        for block1, block2 in zip(self.mixer_blocks1,self.mixer_blocks2):\n",
    "            x = block1(x) + residual1\n",
    "            residual1 = x\n",
    "            x = x.permute(0,2,1)\n",
    "            \n",
    "            x = block2(x) + residual2\n",
    "            residual2 = x\n",
    "            x = x.permute(0,2,1)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        \n",
    "        if self.is_text:\n",
    "            c = self.clin1(x)\n",
    "            i = self.clin2(x)\n",
    "            i_proj = self.clip_proj(i.reshape(len(i), -1, 768))\n",
    "            c = c.reshape(len(c), -1, self.clip_size)\n",
    "            return c, i_proj\n",
    "\n",
    "        if self.is_pool:\n",
    "            c_p = self.clin3(x)\n",
    "            return c_p\n",
    "\n",
    "\n",
    "        \n",
    "b_text = BrainNetwork(text=True, pool=False, h=4096, in_dim=2048, seq_len=1, clip_size=2048, out_dim=2048*77).cuda()\n",
    "# b_pool = BrainNetwork(text=False, pool=True, h=hidden_dim, in_dim=2048, seq_len=seq_len, clip_size=2048, out_dim=2048*77).cuda()\n",
    "utils.count_params(b_text)\n",
    "# utils.count_params(b_pool)\n",
    "\n",
    "# test that the model works on some fake data\n",
    "b = torch.randn((2,1,4096)).cuda()\n",
    "print(\"b.shape\",b.shape)\n",
    "emb, img = b_text(b)\n",
    "print(emb.shape, img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a129fbc7-8667-4c9b-bfda-3df7979142e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
